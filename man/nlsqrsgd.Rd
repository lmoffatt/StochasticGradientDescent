% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nlsgd.R
\name{nlsqrsgd}
\alias{nlsqrsgd}
\title{Stochastic Descent with ADAM step learning
Minimizes the square sum of f(x,xdata), taking samples of xdata for calculating the steps}
\usage{
nlsqrsgd(
  f,
  x0,
  xdata,
  ydata,
  number_of_chunks,
  maxiter = 10000,
  is_complex = FALSE,
  tol_gradient = 1e-05,
  tol_x = 1e-05,
  return_Ft = FALSE,
  gradient = NULL,
  Jacobian = NULL,
  use_gradient = FALSE,
  alpha = 0.001,
  beta1 = 0.9,
  beta2 = 0.999,
  eps = 1e-08,
  eps_J = 1e-05
)
}
\arguments{
\item{f}{function to be optimized}

\item{x0}{initial guess of the multidimentional parameter to be optimized}

\item{xdata}{independent possibly multivariate variable where f is applied}

\item{ydata}{rowise monovariate variable given for each xdata}

\item{number_of_chunks}{number of samples from the data}

\item{maxiter}{maximum number of iteration}

\item{tol_gradient}{termination tolerance for the inf norm of the gradient}

\item{tol_x}{termination tolerance for changes in x}

\item{return_Ft}{whether the function values are returned}

\item{Jacobian}{optional, function that produces the Jacobian of f}

\item{alpha}{gradient descent step size}

\item{beta1}{exponential decay rates for the mean estimates}

\item{beta2}{exponential decay rates for the variance estimates}

\item{eps}{minimum std estimate}

\item{eps_J}{epsilon for Jacobian}
}
\value{
a list containing the optimal x and other things
}
\description{
Stochastic Descent with ADAM step learning
Minimizes the square sum of f(x,xdata), taking samples of xdata for calculating the steps
}
\examples{

# It applies the algorithm described in https://arxiv.org/pdf/1412.6980.pdf

sum_of_Normals <- function(x, X){
  return(x["A1"] * exp(-(X[,"x1"] - x["mean1"])^2/x["sd1"]^2) +
         x["A2"] * exp(-(X[,"x2"] - x["mean2"])^2/x["sd2"]^2))
}

x = c(A1 = 12.3,mean1 = 24, sd1 = 17.3, A2 = 0.23,mean2 = 21.2 ,sd2 = 5)
X=cbind(runif(1e5) * 100 - 20, runif(1e5) * 100 - 20)
colnames(X)<-c('x1','x2')

Yn = sum_of_Normals(x,X)+rnorm(1e5)*0.2


opt=nlsqrsgd(f =sum_of_Normals, x0 = x, xdata = X, ydata = Yn,number_of_chunks = 100)

}
